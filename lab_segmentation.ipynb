{"cells":[{"cell_type":"markdown","metadata":{"id":"HXk9FnTpOzNQ"},"source":["# Сегментация"]},{"cell_type":"markdown","metadata":{"id":"qHrM4UBWPXUl"},"source":["## Лекция"]},{"cell_type":"markdown","metadata":{"id":"raft6xDxkdEw"},"source":["![image.png](https://viso.ai/wp-content/uploads/2021/03/image-segmentation-example-1060x397.jpg)"]},{"cell_type":"markdown","metadata":{"id":"clbqFPEIPPKf"},"source":["1. Сегментация = попиксельная классификация.\n","2. Не требует большого объема тренировочных данных (каждый пиксель картинки - класс)\n","3. Все сегментационные сети - это сети архитектуры FCN (Fully Convolutional Network)"]},{"cell_type":"markdown","metadata":{"id":"TWdUMKMkUZDx"},"source":["Для обучения модели бинарной классификации хватило бы датасета из 100 изображений."]},{"cell_type":"markdown","metadata":{"id":"OzykAdCkt0yP"},"source":["<img src=\"https://mindy-support.com/wp-content/webp-express/webp-images/doc-root/wp-content/uploads/2022/10/types-of-image-segmentation-1.jpg.webp\" alt=\"Drawing\" width=\"85%\"/>"]},{"cell_type":"markdown","metadata":{"id":"nW_wSoBhuEnt"},"source":["#### Типы сегментации изображений\n","\n","1. Semantic image segmentation — предполагает расположение пикселей в изображении на основе семантического класса.\n","\n","2. Instance segmentation — этот метод включает классификацию пикселей на основе экземпляров объекта, а не классов.\n","\n","3. Panoptic segmentation — паноптическая сегментация является более новым методом, чем предыдущие два, упомянутые выше, и часто выражается как комбинация семантической сегментации и сегментации по экземплярам. Она предсказывает идентичность каждого объекта, разделяя каждый экземпляр каждого объекта на изображении."]},{"cell_type":"markdown","metadata":{"id":"5kQaneFjlAvq"},"source":["### Fully Convolutional Network\n","\n","1. Убрать Linear слой, или Linear -> Conv\n","2. В сети мало параметров (обычно все вычисления происходят в Conv слоях, а параметры хранятся в Linear)  \n","3. Берет на вход картинки любого размера"]},{"cell_type":"markdown","metadata":{"id":"NBQ3XjfdltsD"},"source":["FCN8\n","* грубо увеличиваем признаки в большую картинку\n","![image.png](https://production-media.paperswithcode.com/methods/new_alex-model.jpg)"]},{"cell_type":"markdown","metadata":{"id":"24wOnPuamxR5"},"source":["FCN = Efficient Sliding Window"]},{"cell_type":"markdown","metadata":{"id":"qe3C2tp2_Dmu"},"source":["Например, у нас в датасете загружены несколько классов - облако, человек, дом, вода и тому подобное. Мы берем небольшую часть картинки и предсказываем, что представляет из себя большая часть вырезанной картинки. Далее мы делаем меньшие штрихи, и предсказываем более точно, тем самым деля всю картинку на классы объектов. Есть проблема - это неэффективно. FCN делает примерно ту же самую работу, только гораздо эффективнее."]},{"cell_type":"markdown","metadata":{"id":"thH1HoDnnygI"},"source":["![ChessUrl](https://b2633864.smushcdn.com/2633864/wp-content/uploads/2015/03/sliding-window-animated-adrian.gif?lossy=2&strip=1&webp=1 \"chess\")"]},{"cell_type":"markdown","metadata":{"id":"CjiGaggsAnB_"},"source":["<img src=\"https://i.imgur.com/xoZI5hK.png\" alt=\"Drawing\" width=\"65%\"/>"]},{"cell_type":"markdown","metadata":{"id":"Yb6kobRxBqSt"},"source":["#### Classification to Segmentation\n","\n","1. Убрать полносвязные слои\n","2. Добавить декодер"]},{"cell_type":"markdown","metadata":{"id":"11XsHaX0LNJW"},"source":["SegNet (иерархический upsampling (мини апсемплинг)) (примерно 2014)\n","* медленнее чем FCN8, но точнее результат\n","* позволяют работать с любым размером картинок (больше картинка -> лучше)\n","<img src=\"https://production-media.paperswithcode.com/methods/segnet_Vorazx7.png\" alt=\"Drawing\" width=\"100%\"/>"]},{"cell_type":"markdown","metadata":{"id":"O-uUifK5NN9v"},"source":["UNet (добавили в SegNet skip-connections) (2015) (most used in competitions)\n","* не добавляет новых параметров\n","* более точный результат\n","* универсальна (подходит почти под все задачи сегментации)\n","<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20220614121231/Group14.jpg\" alt=\"Drawing\" width=\"100%\"/>\n","\n","Skip-connections - модель пропускает некоторые слои нейронной сети и передает выходные данные одного слоя в качестве входных данных следующим слоям.\n","Преимущества Skip-connections:\n","1. позволяют градиенту протекать пропуская слои\n","2. уточняют, где протекает граница деления классов (точнее граница деления)"]},{"cell_type":"markdown","metadata":{"id":"Szn5dzbPUdA3"},"source":["Частота использования FCN - https://paperswithcode.com/method/fcn"]},{"cell_type":"markdown","metadata":{"id":"15kp7bxZXASG"},"source":["TernasusNet (pretrained UNet)\n","\n","<img src=\"https://camo.githubusercontent.com/790b10b4232e4c1b4cab99cc0d96668fe978184c0e4ad581a1da1f361fead886/68747470733a2f2f686162726173746f726167652e6f72672f776562742f68752f6a692f69722f68756a696972767067706637657377713838685f783761686c69772e706e67\" alt=\"Drawing\" width=\"100%\"/>"]},{"cell_type":"markdown","metadata":{"id":"x5fRbSBWXTAl"},"source":["<img src=\"https://camo.githubusercontent.com/cb252e3355b7cac10e8883f7de5042f4cb1a89ed1bd0968d3b6acd592ed697c2/68747470733a2f2f686162726173746f726167652e6f72672f776562742f6e6f2f75702f78712f6e6f75707871716b5f69767177763365376274797874656d74306d2e706e67\" alt=\"Drawing\" width=\"55%\"/>"]},{"cell_type":"markdown","metadata":{"id":"Wc6nPVhga_Qv"},"source":["<img src=\"https://i.imgur.com/Ek8kmvL.png\" alt=\"Drawing\" width=\"95%\"/>"]},{"cell_type":"markdown","metadata":{"id":"tpOOwIXLkEOi"},"source":["### Метрики"]},{"cell_type":"markdown","metadata":{"id":"RF2rd1rDqNbn"},"source":["<img src=\"https://i.imgur.com/g3ahjUi.png\" alt=\"Drawing\" width=\"50%\"/>"]},{"cell_type":"markdown","metadata":{"id":"huXUtx9xAHTs"},"source":["<img src=\"https://i.imgur.com/mn1Kz9j.png\" alt=\"Drawing\" width=\"35%\"/>"]},{"cell_type":"markdown","metadata":{"id":"y1dN63WgimhL"},"source":["<img src=\"https://i.imgur.com/jLbnoxt.png\" alt=\"Drawing\" width=\"100%\"/>"]},{"cell_type":"markdown","metadata":{"id":"8UM5BhDtb0uV"},"source":["### Feature Pyramid Network (FPN)"]},{"cell_type":"markdown","metadata":{"id":"rGFhZcvWcfqk"},"source":["Алгоритм FPN\n","* легко добавить в архитектуры\n","* помогает с multiscale"]},{"cell_type":"markdown","metadata":{"id":"spMhz_U2cNSm"},"source":["<img src=\"https://miro.medium.com/v2/resize:fit:2000/1*XOumTDx4QEZ6qd_z1Rgf5g.png\" alt=\"Drawing\" width=\"100%\"/>"]},{"cell_type":"markdown","metadata":{"id":"vlX7kbrOdcrq"},"source":["<img src=\"https://i.imgur.com/MmCZnZI.png\" alt=\"Drawing\" width=\"100%\"/>"]},{"cell_type":"markdown","metadata":{"id":"6-NF_NlVjaM6"},"source":["### Detection\n"]},{"cell_type":"markdown","metadata":{"id":"bdgmW__Ytt7p"},"source":["<img src=\"https://av-eks-blogoptimized.s3.amazonaws.com/instance_segmentation_example.jpg\" alt=\"Drawing\" width=\"85%\"/>"]},{"cell_type":"markdown","metadata":{"id":"sEvBEk8wlq8U"},"source":["Decetion - предсказываем много боксов, а потом фильтруем до одного основного."]},{"cell_type":"markdown","metadata":{"id":"jX0VJ9tGmNOP"},"source":["<img src=\"https://i.imgur.com/jAK8btG.png\" alt=\"Drawing\" width=\"80%\"/>"]},{"cell_type":"markdown","metadata":{"id":"WiBqnlshjnTK"},"source":["Предсказываем объекты\n","* класс\n","* координаты объекта\n","* дальность до объекта (опционально)"]},{"cell_type":"markdown","metadata":{"id":"GBOV18wljcHL"},"source":["YOLO5\n","<img src=\"https://upload.wikimedia.org/wikipedia/commons/0/04/Yolo5.gif\" alt=\"Drawing\" width=\"100%\"/>"]},{"cell_type":"markdown","metadata":{"id":"uGzUXXiMkaPF"},"source":["### Detection Metrics"]},{"cell_type":"markdown","metadata":{"id":"lh1UiOs0kIjA"},"source":["\n","<img src=\"https://blog.jetbrains.com/wp-content/uploads/2022/01/dog.jpg\" alt=\"Drawing\" width=\"75%\"/>"]},{"cell_type":"markdown","metadata":{"id":"nVzbBXdQkh6d"},"source":["##### mAP (min average precision)\n","Сводит к одному значению:\n","1. Точность блока объекта (бокса)\n","2. Уверенность мажоритарного класса (class confidence)"]},{"cell_type":"markdown","metadata":{"id":"w27kQhN1o2ZA"},"source":["\n","<img src=\"https://i.imgur.com/tnX8zwJ.png\" alt=\"Drawing\" width=\"75%\"/>"]},{"cell_type":"markdown","metadata":{"id":"zaGIA-PgpEbC"},"source":["### One-shot detection"]},{"cell_type":"markdown","metadata":{"id":"pfnP5n8XpWXG"},"source":["Предсказываем больший блок по точке.\n","Предсказываем координаты бокса и класс объекта с центром в ячейке."]},{"cell_type":"markdown","metadata":{"id":"6G2GgkFupSIW"},"source":["\n","<img src=\"https://i.imgur.com/aaRL49u.png\" alt=\"Drawing\" width=\"75%\"/>"]},{"cell_type":"markdown","metadata":{"id":"ztpEzZNdpu09"},"source":["### YOLO (You Only Look Once)\n","\n","\n","<img src=\"https://i.imgur.com/jIUMxwD.png\" alt=\"Drawing\" width=\"85%\"/>"]},{"cell_type":"markdown","metadata":{"id":"51MDk44mqMVc"},"source":["<img src=\"https://i.imgur.com/oJU52g6.png\" alt=\"Drawing\" width=\"100%\"/>"]},{"cell_type":"markdown","metadata":{"id":"AHvD1lmmqbbE"},"source":["### Two-shot detection"]},{"cell_type":"markdown","metadata":{"id":"rA8lpgrMqswg"},"source":["Задача разбивается в две подзачади\n","1. Определяем потенциальные регионы, где могут быть объекты\n","2. Если объекты есть, то определяется класс объекта"]},{"cell_type":"markdown","metadata":{"id":"s-sJzfR5q-5f"},"source":["В базовом виде такая архитектура работает очень медленно, поэтому были предложены улучшенные версии."]},{"cell_type":"markdown","metadata":{"id":"-fn6u7JBqk6g"},"source":["<img src=\"https://i.imgur.com/dt5pIll.png\" alt=\"Drawing\" width=\"85%\"/>"]},{"cell_type":"markdown","metadata":{"id":"4QwP5difrTXi"},"source":["<img src=\"https://i.imgur.com/ZENNTxI.png\" alt=\"Drawing\" width=\"85%\"/>"]},{"cell_type":"markdown","metadata":{"id":"RbGyI4ufr9A2"},"source":["Преимущества:\n","* End-to-End обучение\n","* Гораздо эффективнее, но точность не выросла\n","* Работает больше как one-shot detection (YOLO)\n","* 5 кадров в секунду"]},{"cell_type":"markdown","metadata":{"id":"Mj_xK8XkruGw"},"source":["<img src=\"https://i.imgur.com/gi8rn0t.png\" alt=\"Drawing\" width=\"85%\"/>"]},{"cell_type":"markdown","metadata":{"id":"JQXjoM-zPcMZ"},"source":["## Работа на семинаре"]},{"cell_type":"markdown","metadata":{"id":"AiFX7_fqwBX9"},"source":["### Напишем UNet модель from scratch (с нуля)"]},{"cell_type":"markdown","metadata":{"id":"yvP7Oh77z5Mr"},"source":["<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20220614121231/Group14.jpg\" alt=\"Drawing\" width=\"85%\"/>"]},{"cell_type":"markdown","metadata":{"id":"Eixkgfy4u8XJ"},"source":["Датасет из соревнования - https://www.kaggle.com/c/carvana-image-masking-challenge"]},{"cell_type":"markdown","metadata":{"id":"1x8FD3FA2u2J"},"source":["Цель - сегментировать автомобиль от окружения\n","1. Изначальные данные - фотографии автомобилей со всех сторон\n","2. Результат - отделенные машины от окружения (binary-classification). Машина - белое, окружение - черное."]},{"cell_type":"markdown","metadata":{"id":"QJ3efJelzYFk"},"source":["#### Загрузка данных №1 (целиком, необязательно)"]},{"cell_type":"markdown","metadata":{"id":"SUSQVsLtxLXW"},"source":["1. Зайти на kaggle -> settings\n","2. Найти блок про API\n","3. Кликните на Expire Token, затем на Create New Token. На ваш компютер загрузится файл kaggle.json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kqbZkZ7ROqu1"},"outputs":[],"source":["! pip install -q kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NZ2hFTLtxIda"},"outputs":[],"source":["from google.colab import files"]},{"cell_type":"markdown","metadata":{"id":"_MOCtalFxzT5"},"source":["4. Загрузите, нажав на ячейку ниже, файл в Colab."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gKyvDQoyxK3G"},"outputs":[],"source":["files.upload()"]},{"cell_type":"markdown","metadata":{"id":"JOyry2kMx8V0"},"source":["5. Выдайте права для kaggle, прогнав ячейки ниже\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o5NMAO9Ix8AM"},"outputs":[],"source":["! mkdir ~/.kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CG3xBWELyG9z"},"outputs":[],"source":["! cp kaggle.json ~/.kaggle/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N2cBVUyByJYN"},"outputs":[],"source":["! chmod 600 ~/.kaggle/kaggle.json # выдает права для файла"]},{"cell_type":"markdown","metadata":{"id":"Jx4L3WzmyO7c"},"source":["6. Загрузите используемый датасет. Перед загрузкой вам необходимо принять правила соревнования на kaggle, для этого по ссылке https://www.kaggle.com/competitions/carvana-image-masking-challenge/data перейдите на kaggle и нажмите Join Competition\n"]},{"cell_type":"markdown","metadata":{"id":"VcD9WxAFzFKO"},"source":["* Датасет весит в запакованном виде 24,4 Гб, его загрузка займет некоторое время (~5-7 минут).\n","* Не рекомендуется скачивать на личные компьютеры из-за низкой скорости передачи.\n","* Вы можете скачать только часть датасета (он разделен на высокое разрешение и низкое)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u44KX78JyMfl"},"outputs":[],"source":["! kaggle competitions download -c carvana-image-masking-challenge"]},{"cell_type":"markdown","metadata":{"id":"TRQxVEBg2Y2P"},"source":["Разархивирование датасета (~5-7 минут)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wkq2jLVi2SQR"},"outputs":[],"source":["! unzip carvana-image-masking-challenge.zip"]},{"cell_type":"markdown","metadata":{"id":"oNzDzfcC5TFG"},"source":["#### Загрузка данных №2 (только train, предпочтительный вариант)"]},{"cell_type":"markdown","metadata":{"id":"VdwCEpM3_7qk"},"source":["Можно сохранить на Google Drive, потом подключить драйв и не скачивать файлы"]},{"cell_type":"markdown","metadata":{"id":"M3y8M8OO5egJ"},"source":["Датасет - https://drive.google.com/drive/folders/1t8QESPcEwsR6VBdGhdMNAqJJCjIhdSIv?usp=sharing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kvRmoS2eT0Vs","outputId":"9b70eb00-f469-4f05-9163-941edd36a185"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VEiHKbD9Ca6l"},"outputs":[],"source":["! unzip 'drive/MyDrive/lab_segmentation_dataset/train_images.zip'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q53p14QTCkTu"},"outputs":[],"source":["! unzip 'drive/MyDrive/lab_segmentation_dataset/val_images.zip'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8WBseg02Crv6"},"outputs":[],"source":["! unzip 'drive/MyDrive/lab_segmentation_dataset/val_masks.zip'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bu5i6J2mCvxT"},"outputs":[],"source":["! unzip 'drive/MyDrive/lab_segmentation_dataset/train_masks.zip'"]},{"cell_type":"markdown","metadata":{"id":"Swbp9EcL1hqH"},"source":["#### Архитектура модели"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ff0GkqWaypKR"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms.functional as TF\n","\n","# Определение блока двойной свертки\n","class DoubleConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(DoubleConv, self).__init__()\n","        self.conv = nn.Sequential(\n","            # Первая свертка: in_channels -> out_channels\n","            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            # Вторая свертка: out_channels -> out_channels\n","            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","        )\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","# Определение модели U-Net\n","class UNET(nn.Module):\n","    def __init__(\n","            self, in_channels=3, out_channels=1, features=[64, 128, 256, 512],\n","    ):\n","        super(UNET, self).__init__()\n","        self.ups = nn.ModuleList()\n","        self.downs = nn.ModuleList()\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        # Создание блоков свертки для слоев \"вниз\" в U-Net\n","        for feature in features:\n","            self.downs.append(DoubleConv(in_channels, feature))\n","            in_channels = feature\n","\n","        # Создание блоков транспонированной свертки и блоков двойной свертки для слоев \"вверх\" в U-Net\n","        for feature in reversed(features):\n","            self.ups.append(\n","                nn.ConvTranspose2d(\n","                    feature*2, feature, kernel_size=2, stride=2,\n","                )\n","            )\n","            self.ups.append(DoubleConv(feature*2, feature))\n","\n","        # Последний блок двойной свертки перед финальной сверткой\n","        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n","        # Финальная свертка для получения окончательного результата\n","        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        skip_connections = []\n","\n","        # Проход \"вниз\" по сети U-Net\n","        for down in self.downs:\n","            x = down(x)\n","            skip_connections.append(x)\n","            x = self.pool(x)\n","\n","        # Проход \"вверх\" по сети U-Net\n","        x = self.bottleneck(x)\n","        skip_connections = skip_connections[::-1]\n","\n","        for idx in range(0, len(self.ups), 2):\n","            x = self.ups[idx](x)\n","            skip_connection = skip_connections[idx//2]\n","\n","            # Изменение размера восходящего пути, чтобы он соответствовал размеру пропуска\n","            if x.shape != skip_connection.shape:\n","                x = TF.resize(x, size=skip_connection.shape[2:])\n","\n","            # Конкатенация восходящего и нисходящего путей\n","            concat_skip = torch.cat((skip_connection, x), dim=1)\n","            x = self.ups[idx+1](concat_skip)\n","\n","        return self.final_conv(x)\n","\n","# Функция для тестирования модели\n","def test():\n","    x = torch.randn((3, 1, 161, 161))\n","    model = UNET(in_channels=1, out_channels=1)\n","    preds = model(x)\n","    assert preds.shape == x.shape"]},{"cell_type":"markdown","metadata":{"id":"AfLs0moz9MFE"},"source":["#### Датасет"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O9EBWsjy9JqP"},"outputs":[],"source":["import os\n","from PIL import Image\n","from torch.utils.data import Dataset\n","import numpy as np\n","\n","class CarvanaDataset(Dataset):\n","    def __init__(self, image_dir, mask_dir, transform=None):\n","        # Путь к директории с изображениями и масками\n","        self.image_dir = image_dir\n","        self.mask_dir = mask_dir\n","        self.transform = transform\n","        # Список файлов изображений в директории\n","        self.images = os.listdir(image_dir)\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, index):\n","        # Получение пути к изображению и соответствующей маске\n","        img_path = os.path.join(self.image_dir, self.images[index])\n","        mask_path = os.path.join(self.mask_dir, self.images[index].replace(\".jpg\", \"_mask.gif\"))\n","\n","        # Загрузка изображения и маски, преобразование их в массивы NumPy\n","        image = np.array(Image.open(img_path).convert(\"RGB\"))\n","        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n","\n","        # Преобразование значений маски для сегментации (приведение к диапазону [0, 1])\n","        mask[mask == 255.0] = 1.0\n","\n","        # Применение трансформаций (если они заданы)\n","        if self.transform is not None:\n","            augmentations = self.transform(image=image, mask=mask)\n","            image = augmentations[\"image\"]\n","            mask = augmentations[\"mask\"]\n","\n","        return image, mask"]},{"cell_type":"markdown","metadata":{"id":"r8ZBmq29-3Na"},"source":["#### Вспомогательные функции (обязательно)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gmIjcb3u-739"},"outputs":[],"source":["import torch\n","import torchvision\n","from torch.utils.data import DataLoader\n","\n","def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n","    print(\"Saving checkpoint...\")\n","    torch.save(state, filename)\n","\n","def load_checkpoint(checkpoint, model):\n","    print(\"Loading checkpoint...\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","\n","def get_loaders(\n","    train_dir,\n","    train_maskdir,\n","    val_dir,\n","    val_maskdir,\n","    batch_size,\n","    train_transform,\n","    val_transform,\n","    num_workers=4,\n","    pin_memory=True,\n","):\n","    train_ds = CarvanaDataset(\n","        image_dir=train_dir,\n","        mask_dir=train_maskdir,\n","        transform=train_transform,\n","    )\n","\n","    train_loader = DataLoader(\n","        train_ds,\n","        batch_size=batch_size,\n","        num_workers=num_workers,\n","        pin_memory=pin_memory,\n","        shuffle=True,\n","    )\n","\n","    val_ds = CarvanaDataset(\n","        image_dir=val_dir,\n","        mask_dir=val_maskdir,\n","        transform=val_transform,\n","    )\n","\n","    val_loader = DataLoader(\n","        val_ds,\n","        batch_size=batch_size,\n","        num_workers=num_workers,\n","        pin_memory=pin_memory,\n","        shuffle=False,\n","    )\n","\n","    return train_loader, val_loader\n","\n","def check_accuracy(loader, model, device=\"cuda\"):\n","    num_correct = 0\n","    num_pixels = 0\n","    dice_score = 0\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x = x.to(device)\n","            y = y.to(device).unsqueeze(1)\n","            preds = torch.sigmoid(model(x))\n","            preds = (preds > 0.5).float()\n","            num_correct += (preds == y).sum()\n","            num_pixels += torch.numel(preds)\n","            dice_score += (2 * (preds * y).sum()) / (\n","                (preds + y).sum() + 1e-8\n","            )\n","\n","    print(\n","        f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\"\n","    )\n","    print(f\"Dice score: {dice_score/len(loader)}\")\n","    model.train()\n","\n","def save_predictions_as_imgs(\n","    loader, model, folder=\"saved_images/\", device=\"cuda\"\n","):\n","    model.eval()\n","    for idx, (x, y) in enumerate(loader):\n","        x = x.to(device=device)\n","        with torch.no_grad():\n","            preds = torch.sigmoid(model(x))\n","            preds = (preds > 0.5).float()\n","        torchvision.utils.save_image(\n","            preds, f\"{folder}/pred_{idx}.png\"\n","        )\n","        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}{idx}.png\")\n","\n","    model.train()"]},{"cell_type":"markdown","metadata":{"id":"AGza3F3z-Br3"},"source":["#### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eDlcRVY39UnK"},"outputs":[],"source":["import torch\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from tqdm import tqdm\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# Гиперпараметры и т. д.\n","LEARNING_RATE = 1e-4\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","BATCH_SIZE = 16\n","NUM_EPOCHS = 3\n","NUM_WORKERS = 2\n","IMAGE_HEIGHT = 160  # 1280 изначально\n","IMAGE_WIDTH = 240  # 1918 изначально\n","PIN_MEMORY = True\n","LOAD_MODEL = False\n","TRAIN_IMG_DIR = \"train_images/\"\n","TRAIN_MASK_DIR = \"train_masks/\"\n","VAL_IMG_DIR = \"val_images/\"\n","VAL_MASK_DIR = \"val_masks/\"\n","\n","def train_fn(loader, model, optimizer, loss_fn, scaler):\n","    loop = tqdm(loader)\n","\n","    for batch_idx, (data, targets) in enumerate(loop):\n","        data = data.to(device=DEVICE)\n","        targets = targets.float().unsqueeze(1).to(device=DEVICE)\n","\n","        # forward\n","        with torch.cuda.amp.autocast():\n","            predictions = model(data)\n","            loss = loss_fn(predictions, targets)\n","\n","        # backward\n","        optimizer.zero_grad()\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        # update tqdm loop\n","        loop.set_postfix(loss=loss.item())\n","\n","\n","def main():\n","    # Аугментации для тренировочного и валидационного датасетов\n","    train_transform = A.Compose(\n","        [\n","            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n","            A.Rotate(limit=35, p=1.0),\n","            A.HorizontalFlip(p=0.5),\n","            A.VerticalFlip(p=0.1),\n","            A.Normalize(\n","                mean=[0.0, 0.0, 0.0],\n","                std=[1.0, 1.0, 1.0],\n","                max_pixel_value=255.0,\n","            ),\n","            ToTensorV2(),\n","        ],\n","    )\n","\n","    val_transforms = A.Compose(\n","        [\n","            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n","            A.Normalize(\n","                mean=[0.0, 0.0, 0.0],\n","                std=[1.0, 1.0, 1.0],\n","                max_pixel_value=255.0,\n","            ),\n","            ToTensorV2(),\n","        ],\n","    )\n","\n","    # Инициализация модели, функции потерь и оптимизатора\n","    model = UNET(in_channels=3, out_channels=1).to(DEVICE)\n","    loss_fn = nn.BCEWithLogitsLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","    train_loader, val_loader = get_loaders(\n","        TRAIN_IMG_DIR,\n","        TRAIN_MASK_DIR,\n","        VAL_IMG_DIR,\n","        VAL_MASK_DIR,\n","        BATCH_SIZE,\n","        train_transform,\n","        val_transforms,\n","        NUM_WORKERS,\n","        PIN_MEMORY,\n","    )\n","\n","    if LOAD_MODEL:\n","        load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model)\n","\n","    # Проверка точности модели на валидационном датасете до начала обучения\n","    check_accuracy(val_loader, model, device=DEVICE)\n","    scaler = torch.cuda.amp.GradScaler()\n","\n","    # Цикл обучения модели\n","    for epoch in range(NUM_EPOCHS):\n","        train_fn(train_loader, model, optimizer, loss_fn, scaler)\n","\n","        # Сохранение модели\n","        checkpoint = {\n","            \"state_dict\": model.state_dict(),\n","            \"optimizer\":optimizer.state_dict(),\n","        }\n","        save_checkpoint(checkpoint)\n","\n","        # Проверка точности на валидационном датасете\n","        check_accuracy(val_loader, model, device=DEVICE)\n","\n","        # Сохранение предсказаний модели в виде изображений\n","        save_predictions_as_imgs(\n","            val_loader, model, folder=\"saved_images/\", device=DEVICE\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AgPT0a1IC-gP","outputId":"9650b478-4447-4928-fafd-f6ac7ade5681"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Got 423093/1843200 with acc 22.95\n","Dice score: 0.3733779788017273\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 315/315 [03:01<00:00,  1.73it/s, loss=0.123]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Saving checkpoint...\n","Got 1822987/1843200 with acc 98.90\n","Dice score: 0.9764560461044312\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 315/315 [02:56<00:00,  1.79it/s, loss=0.0741]\n"]},{"output_type":"stream","name":"stdout","text":["Saving checkpoint...\n","Got 1829250/1843200 with acc 99.24\n","Dice score: 0.9836365580558777\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 315/315 [02:53<00:00,  1.82it/s, loss=0.0536]\n"]},{"output_type":"stream","name":"stdout","text":["Saving checkpoint...\n","Got 1829805/1843200 with acc 99.27\n","Dice score: 0.9841086268424988\n"]}],"source":["main()"]},{"cell_type":"markdown","source":["### Результаты"],"metadata":{"id":"8bBrxbC8hFPn"}},{"cell_type":"markdown","source":["Реальное изображение"],"metadata":{"id":"AoV9aQ8Whg0t"}},{"cell_type":"markdown","source":["<img src=\"https://i.imgur.com/iIJKnYt.png\" alt=\"Drawing\" width=\"100%\"/>"],"metadata":{"id":"SjBx6vwvg-MT"}},{"cell_type":"markdown","source":["Предсказанное"],"metadata":{"id":"BmUC1NG6hj2k"}},{"cell_type":"markdown","source":["<img src=\"https://i.imgur.com/ZSSEQ27.png\" alt=\"Drawing\" width=\"100%\"/>"],"metadata":{"id":"Y72w8SnshOIv"}}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}